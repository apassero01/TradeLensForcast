{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import test_utils_models\n",
    "from importlib import reload\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import Transformer\n",
    "import TransformerV3\n",
    "import TransformerService\n",
    "import TransformerServiceV2\n",
    "import torch.optim as optim\n",
    "import trainer.Trainer as Trainer\n",
    "import torch.nn as nn\n",
    "import test_utils\n",
    "import DataVisualizations\n",
    "import Utilities\n",
    "import models.BuiltModels as BuiltModels\n",
    "import models.decoders\n",
    "import models.encoders\n",
    "import models.ModelScraper\n",
    "import models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'http://localhost:8000/sequenceset_manager/get_sequence_data'\n",
    "\n",
    "features = ['pctChgclose', 'pctChgvolume', 'opHi', 'opCl','hiCl','loCl','opLo',\"pctDiff+ema5_close\", \"pctDiff+ema10_close\", \"pctDiff+ema20_close\",'pctDiff+ema50_close', 'pctDiff+ema100_close',\n",
    "'pctDiff+ema200_close','pctDiff+bb_high_close10', 'pctDiff+bb_low_close10', 'pctDiff+smaVol10_volume', 'pctDiff+smaVol20_volume', 'pctChgClOp', 'macd_signal','bb_indicator20',\"bb_indicator50\",\n",
    "            ]\n",
    "# struct_features = ['open', 'high', 'low', 'close', 'ema20', 'ema50', 'ema100']\n",
    "# struct_features = ['close']\n",
    "struct_features = []\n",
    "# features = ['pctChgclose','pctChgvolume','opHi', 'opCl','hiCl','loCl','opLo']\n",
    "# features = ['pctChgclose']\n",
    "# features = ['close']\n",
    "# target_features = ['pctChgclose+{}'.format(i) for i in range(1, 16) ]\n",
    "# target_features = ['close+{}'.format(i) for i in range(1, 16) ]\n",
    "target_features = ['cumPctChg+{}'.format(i) for i in range(1,16) ]\n",
    "\n",
    "indices_seq = list(range(len(features + struct_features + target_features)))\n",
    "\n",
    "feature_dict = {col: index for col, index in zip(features+struct_features + target_features, indices_seq)}\n"
   ],
   "id": "57ac927fda1812",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(feature_dict)",
   "id": "7757611afbc4bb60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tickers = ['MSFT', 'AAPL', 'AMZN','QQQ','SPY','F', 'TSLA', \"BAC\", \"LLY\", \"PEP\" ]\n",
    "# tickers = ['AAPL', 'MSFT','AMZN','QQQ','SPY']\n",
    "tickers = [\"AAPL\", \"SPY\", \"QQQ\", \"XOM\", \"MSFT\", \"AMZN\", \"BB\", 'F', 'TSLA', 'GE',\n",
    "           \"JPM\", \"BAC\", \"LLY\", \"NKE\", \"WMT\", \"PEP\"]\n",
    "sequence_lengths = [50]\n",
    "all_sequences = []\n",
    "\n",
    "all_params = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    for sequence_length in sequence_lengths:\n",
    "        all_params.append({\n",
    "            'ticker': ticker,\n",
    "            'features': features + struct_features + target_features,\n",
    "            'interval': '1d',\n",
    "            'start_date': '2007-01-01',\n",
    "            'sequence_length': sequence_length\n",
    "        })\n",
    "for params in all_params:\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            all_sequences.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to decode JSON: {e}\")\n",
    "            print(\"Response text:\", response.text)\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")"
   ],
   "id": "559d09e2c0c4af5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ba0a61a118ebc476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(all_sequences))",
   "id": "85779c2cad87264f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(all_sequences[-1][-500]['end_timestamp'])\n",
    "print(all_sequences[-1][-500]['start_timestamp'])\n"
   ],
   "id": "cd73de3b4d9a5b9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(all_sequences[-1][-500]['sliced_data'][-1][feature_dict['pctChgclose']])",
   "id": "c7a17ab30dae462a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_feauture_index = [feature_dict[col] for col in target_features]\n",
    "for index in target_feauture_index:\n",
    "    print(all_sequences[-1][-500]['sliced_data'][-1][index])"
   ],
   "id": "1e7e9ae6a8a246c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_3d_array(stock_sequences, max_len_seq):\n",
    "    # Create a 3D array to store the sequences\n",
    "    # The array will have the shape (num_sequences, sequence_length, num_features)\n",
    "    # stock sequences is a list of sequence_dicts where each sequence_dict is a dictionary\n",
    "    # we are extracting 'sliced_data' from each sequence_dict and creating 3d np array\n",
    "\n",
    "    num_features = len(features) + len(struct_features)\n",
    "\n",
    "    sequence_steps = len(stock_sequences[0]['sliced_data'])\n",
    "\n",
    "    num_sequences = len(stock_sequences)\n",
    "    X = np.zeros((num_sequences, sequence_steps, num_features))\n",
    "    y = np.zeros((num_sequences, len(target_features)))\n",
    "\n",
    "    indices_map = {}\n",
    "    cur_index = 0\n",
    "    for i, sequence_dict in enumerate(stock_sequences):\n",
    "        X_cols = [feature_dict[col] for col in features + struct_features]\n",
    "        y_cols = [feature_dict[col] for col in target_features]\n",
    "\n",
    "        X[i, :, :] = np.array(sequence_dict['sliced_data'])[:, X_cols]\n",
    "        X_padded = np.zeros((num_sequences, max_len_seq, num_features))\n",
    "        X_padded[:, :sequence_steps, :] = X\n",
    "        y[i, :] = np.array(sequence_dict['sliced_data'])[-1, y_cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return X_padded, y"
   ],
   "id": "87626cdad6efb7bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_multi_stock(all_sequences, max_seq_length, test_size=0.1):\n",
    "    X_train = np.zeros((0, 0, 0))\n",
    "    X_test = np.zeros((0, 0, 0))\n",
    "    y_train = np.zeros((0, 0))\n",
    "    y_test = np.zeros((0, 0))\n",
    "    \n",
    "    test_seq_objs = []\n",
    "    train_seq_objs = []\n",
    "    for sequences in all_sequences:\n",
    "        print(len(sequences))\n",
    "        X, y = create_3d_array(sequences,max_seq_length)\n",
    "        print(X.shape)\n",
    "\n",
    "        for i in range(len(X)-1, 0, -1):\n",
    "            if np.isnan(y[i]).any():\n",
    "                X = np.delete(X, i, axis=0)\n",
    "                y = np.delete(y, i, axis=0)\n",
    "\n",
    "        X_train_cur, X_test_cur, y_train_cur, y_test_cur = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "        #TODO need to set train/test start end date and split on time instead of percentage of dataset size\n",
    "        \n",
    "        train_seq_objs += [sequences[i] for i in range(len(X_train_cur))]\n",
    "        test_seq_objs += [sequences[i] for i in range(len(X_train_cur), len(X_train_cur)+len(X_test_cur))]\n",
    "\n",
    "        # scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)\n",
    "        # X_train_cur = scaler.fit_transform(X_train_cur)\n",
    "        # X_test_cur = scaler.transform(X_test_cur)\n",
    "        # \n",
    "        y_scaler = sk.preprocessing.StandardScaler()\n",
    "        # y_train_cur = y_scaler.fit_transform(y_train_cur)\n",
    "        # y_test_cur = y_scaler.transform(y_test_cur)\n",
    "        \n",
    "        # X_train_cur = X_train_cur[0:-25, :, :]\n",
    "        # y_train_cur = y_train_cur[0:-25, :]\n",
    "        # X_test_cur = X_test_cur[0:-25, :, :]\n",
    "        # y_test_cur = y_test_cur[0:-25, :]\n",
    "\n",
    "\n",
    "        if X_train.shape[0] == 0:\n",
    "            X_train = X_train_cur\n",
    "            X_test = X_test_cur\n",
    "            y_train = y_train_cur\n",
    "            y_test = y_test_cur\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X_train_cur), axis=0)\n",
    "            X_test = np.concatenate((X_test, X_test_cur), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train_cur), axis=0)\n",
    "            y_test = np.concatenate((y_test, y_test_cur), axis=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, y_scaler, train_seq_objs, test_seq_objs\n"
   ],
   "id": "3f1357d2ce14d113",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_whole, X_test, y_train_whole, y_test, y_scaler,train_seq_objs, test_seq_objs = preprocess_multi_stock(all_sequences, max(sequence_lengths))",
   "id": "89aebd90a51c9e85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_training_data(X_train, y_train, percentage=0.3):\n",
    "    # Get the number of samples\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    # Calculate the number of samples for 30%\n",
    "    num_samples_to_sample = int(percentage * num_samples)\n",
    "\n",
    "    # Randomly sample indices (without replacement)\n",
    "    sampled_indices = np.random.choice(num_samples, num_samples_to_sample, replace=False)\n",
    "\n",
    "    # Extract the same indices from both X_train and y_train\n",
    "    X_sampled = X_train[sampled_indices]\n",
    "    y_sampled = y_train[sampled_indices]\n",
    "\n",
    "    return X_sampled, y_sampled"
   ],
   "id": "67ca728d11e35920",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## sample random 30% of the sequences for training set and get rid of the rest\n",
    "# X_train, y_train = sample_training_data(X_train_whole, y_train_whole, percentage=1)\n",
    "X_train = X_train_whole\n",
    "y_train = y_train_whole"
   ],
   "id": "1e0a4a859000f40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ],
   "id": "d10807c09be62fb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(feature_dict)",
   "id": "38a30c1275afd314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if len(struct_features) > 0:\n",
    "    X_train_scaled = Utilities.scale_sequence_by_group(struct_features, feature_dict, X_train)\n",
    "    X_test_scaled = Utilities.scale_sequence_by_group(struct_features, feature_dict, X_test)\n",
    "else:\n",
    "    X_train_scaled = X_train\n",
    "    X_test_scaled = X_test"
   ],
   "id": "b587e62543e97732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_indices = [feature_dict[col] for col in features]\n",
    "\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)\n",
    "X_train_scaled[:,:,x_indices] = scaler.fit_transform(X_train[:,:,x_indices])\n",
    "X_test_scaled[:,:,x_indices] = scaler.transform(X_test[:,:,x_indices])\n",
    "\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "# X_train_scaled = X_train\n",
    "# X_test_scaled = X_test\n",
    "# y_train_scaled = y_train\n",
    "# y_test_scaled = y_test"
   ],
   "id": "f70fc92c085d71dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train_scaled = np.expand_dims(y_train_scaled, axis=2)\n",
    "y_test_scaled = np.expand_dims(y_test_scaled, axis=2)\n",
    "print(y_train_scaled.shape, y_test_scaled.shape)"
   ],
   "id": "b62d31936fa9a093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X_train_scaled.shape, X_test_scaled.shape)",
   "id": "3137e3d5d343227c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X_train_scaled[0,:,-1])",
   "id": "974480e985995b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_feature_histograms(X_test_scaled, features + struct_features, feature_dict)\n",
   "id": "e6fa9d9c9a8a3131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_y_feature_histograms(y_train_scaled, target_features)",
   "id": "b5cfafa2de90b207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_y_feature_histograms(y_test_scaled, target_features)",
   "id": "e31fa8088e8aa850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_y_feature_histograms(np.expand_dims(y_scaler.inverse_transform(y_train_scaled.squeeze()),-1), target_features)\n",
   "id": "d362a2e3b3e18d64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_y_feature_histograms(np.expand_dims(y_train,-1), target_features)\n",
   "id": "889eb6dddffc5a62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reload(DataVisualizations)",
   "id": "be9b4063afef1868",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    print(\"MPS is available. Setting as default device.\")\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.set_default_device(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "batch_size = 64\n",
    "\n",
    "# Convert data to tensors and move them to the appropriate device\n",
    "print(X_train_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "# X_train_scaled_concat = np.concatenate((X_train_scaled, y_train_scaled), axis=-1)\n",
    "# print(X_train_scaled_concat.shape)\n",
    "# X_test_scaled_concat = np.concatenate((X_test_scaled, y_test_scaled), axis=-1)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "print(X_train_tensor.device, X_test_tensor.device, y_train_tensor.device, y_test_tensor.device)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "f6ca04281a671546",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape)",
   "id": "6d0644c05552d008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_layers = 2\n",
    "d_model = 32\n",
    "num_heads = 2\n",
    "d_ff = 64\n",
    "input_dim = X_train_tensor.shape[2]  # Number of features in X_train\n",
    "# output_dim = 1  # Since y_train has shape (batch, output_steps, 1)\n",
    "output_dim = y_train_tensor.shape[1]\n",
    "# Initialize the Transformer models\n",
    "model_obj = BuiltModels.Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    encoder_input_dim=input_dim,\n",
    "    decoder_input_dim=output_dim,\n",
    "    dropout=.2\n",
    ").to(device)\n",
    "print(next(model_obj.parameters()).device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'The model has {count_parameters(model_obj):,} trainable parameters')"
   ],
   "id": "79a8d5461c27c4f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model_obj.parameters(), lr=0.001)\n",
    "# base_optimizer = optim.Adam\n",
    "# optimizer = torch.optim.SGD(models.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# optimizer = Transformer.SAM(models.parameters(), base_optimizer, rho=0.3, lr=0.01)\n",
    "# criterion = nn.MSELoss()\n",
    "# get mae criterian \n",
    "# criterion = nn.L1Loss()\n",
    "# criterion = nn.HuberLoss(delta=1.0)\n",
    "criterion = Transformer.gaussian_nll_loss\n",
    "# Initialize the Trainer with the device\n",
    "trainer_obj = Trainer.Trainer(\n",
    "    model=model_obj,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    max_length=2,\n",
    "    start_token_value=-999 # Start token value, adjust if necessary\n",
    ")"
   ],
   "id": "b33c5cbe9afb9401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from importlib import reload\n",
    "reload(BuiltModels)\n",
    "reload(models.decoders)\n",
    "reload(models.encoders)\n",
    "reload(models.ModelScraper)\n",
    "reload(Trainer)\n"
   ],
   "id": "357135dcee39cbb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# file_path = \"saved_models/model_weights20240929_113227.pth\"\n",
    "# model_obj.load_state_dict(torch.load(file_path, map_location=device))"
   ],
   "id": "6df83de9aa211005",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-29T22:03:21.928375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 250\n",
    "\n",
    "# Train the models\n",
    "\n",
    "trainer_obj.fit(train_dataloader, test_dataloader, epochs=epochs, clip_value=None)"
   ],
   "id": "db634b78cdd27d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.850 | Val Loss: 1.922\n",
      "Epoch 2 | Train Loss: 0.851 | Val Loss: 1.890\n",
      "Epoch 3 | Train Loss: 0.852 | Val Loss: 1.948\n",
      "Epoch 4 | Train Loss: 0.852 | Val Loss: 1.899\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gradient_test_X = X_train_tensor[-64:]\n",
    "gradient_test_y = y_train_tensor[-64:]\n",
    "\n",
    "gradients = trainer_obj.log_gradients_with_names(gradient_test_X, gradient_test_y)\n",
    "DataVisualizations.visualize_gradients(gradients)"
   ],
   "id": "1d7f4c48961cf499",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# y_pred = trainer_obj.predict(X_test_tensor[:256])\n",
    "y_pred = trainer_obj.predict(X_test_tensor)\n",
    "print(y_pred.shape)"
   ],
   "id": "f0eba76faeee1545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaling_factor = 1\n",
    "y_pred = y_pred * scaling_factor\n",
    "mu_pred = y_pred[:, :, 0]\n",
    "log_var_pred = y_pred[:, :, 1]\n",
    "variance_pred = torch.exp(log_var_pred)\n",
    "std_pred = torch.sqrt(variance_pred)\n",
    "print(mu_pred.shape, std_pred.shape)\n"
   ],
   "id": "28d0ebb7af460a19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mu_pred_denorm = y_scaler.inverse_transform(mu_pred.detach().cpu().numpy())\n",
    "std_pred_denorm = y_scaler.inverse_transform(std_pred.detach().cpu().numpy())\n",
    "# mu_pred_denorm = mu_pred.detach().cpu().numpy()\n",
    "# std_pred_denorm = std_pred.detach().cpu().numpy()"
   ],
   "id": "51f3c5243797b690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer_obj.generate_model_architecture()",
   "id": "411ff0d2181a4e6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# retrieval_list = [\"transformer_encoder:encoder_layer0:multi_head_attention\",\n",
    "# \"transformer_encoder:encoder_layer1:multi_head_attention\",\n",
    "# \"transformer_decoder:decoder_layer0:self_attention\",\n",
    "# \"transformer_decoder:decoder_layer0:cross_attention\",\n",
    "# \"transformer_decoder:decoder_layer1:self_attention\",\n",
    "# \"transformer_decoder:decoder_layer1:cross_attention\"]\n",
    "retrieval_list = [\n",
    "    'transformer_encoder:encoder_layer0',\n",
    "    'transformer_encoder:encoder_layer0:multi_head_attention',\n",
    "    'transformer_encoder:encoder_layer1',\n",
    "    'transformer_encoder:encoder_layer1:multi_head_attention',\n",
    "    'DecoderRNN'\n",
    "\n",
    "]\n",
    "output_dict = trainer_obj.scrape_model(retrieval_list)"
   ],
   "id": "b5490160af87fa26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for key in output_dict.keys():\n",
    "    print(key)\n",
    "    print(output_dict[key].keys())"
   ],
   "id": "c1b6a2a2e3d8927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(output_dict['transformer_encoder:encoder_layer0:multi_head_attention'].shape)\n",
    "print(len(output_dict))"
   ],
   "id": "f40b0d5272278336",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2b0e7761631a0262",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(y_pred.shape)",
   "id": "4039b1a0f9ca539a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reload(test_utils)\n",
    "reload(Transformer)\n",
    "reload(TransformerService)\n",
    "reload(DataVisualizations)\n",
    "reload(BuiltModels)\n",
    "reload(Trainer)\n",
    "reload(models.encoders)\n",
    "reload(models.ModelScraper)\n",
    "reload(models.decoders)"
   ],
   "id": "ac8c3d7a092126fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mu_pred = mu_pred_denorm.squeeze()\n",
    "std_pred = std_pred_denorm.squeeze()"
   ],
   "id": "82944ede155d8ee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(mu_pred.shape)",
   "id": "32d86fe4e9c36666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output_string, results, predicted_y_old = test_utils.eval_model(y_test, mu_pred, num_days = y_pred.shape[1], alreadyCum=True)",
   "id": "d3ce59324e455851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(output_string)",
   "id": "bf3b8972a5b091e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_utils.visualize_future_distribution(results)",
   "id": "783da5c5b2d5bbcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.visualize_step_distribution(mu_pred,y_test)",
   "id": "4e2ea6c32b37f1f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize variance\n",
    "DataVisualizations.visualize_std(std_pred)"
   ],
   "id": "68bc51a98fe5acb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_residuals(y_test ,mu_pred, std_pred)",
   "id": "1deb4775842537f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DataVisualizations.plot_layer_activations(model_obj, X_test_tensor)",
   "id": "f0c4af49913a9eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T22:02:23.761476Z",
     "start_time": "2024-09-29T22:02:23.710654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(500, 510):\n",
    "\n",
    "    # print(test_seq_objs[i]['ticker'])\n",
    "    print(test_seq_objs[i]['start_timestamp'])\n",
    "    print(test_seq_objs[i]['end_timestamp'])\n",
    "    print(y_test[i])\n",
    "    DataVisualizations.layer_heatmap(output_dict, i)\n",
    "    DataVisualizations.visualize_future_predictions_with_uncertainty(y_train[i], mu_pred[i], std_pred[i], already_cumulative=True)"
   ],
   "id": "4b14bcabc62d7654",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-07T00:00:00Z\n",
      "2023-02-17T00:00:00Z\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ticker'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[215], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(test_seq_objs[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(test_seq_objs[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend_timestamp\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(test_seq_objs[i][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mticker\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(y_test[i])\n\u001B[1;32m      8\u001B[0m DataVisualizations\u001B[38;5;241m.\u001B[39mlayer_heatmap(output_dict, i)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'ticker'"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "82f55281d9637942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# embeddings = DataVisualizations.extract_embeddings(models, X_test_tensor[:1000])\n",
    "# DataVisualizations.visualize_embeddings(embeddings, method='tsne', perplexity=50)"
   ],
   "id": "3189105a8ba946f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "343246a9d0339e97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# add date/time to models name\n",
    "model_name = 'model_weights' + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(model_obj.state_dict(), 'saved_models/' + model_name + '.pth')"
   ],
   "id": "97a03ceeff16ac44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5ba008f9734fd20",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
